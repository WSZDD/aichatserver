#include "napi/native_api.h"
#include "llama.h"
#include <string>
#include <vector>
#include <cstdio>
#include <cstring>
#include <hilog/log.h>
#include <thread>
#include <mutex>
#include <atomic>
#include <unistd.h>

#undef LOG_DOMAIN
#undef LOG_TAG
#define LOG_DOMAIN 0x0000
#define LOG_TAG "MNN_NATIVE"
#define LOGI(...) OH_LOG_Print(LOG_APP, LOG_INFO, LOG_DOMAIN, LOG_TAG, __VA_ARGS__)
#define LOGE(...) OH_LOG_Print(LOG_APP, LOG_ERROR, LOG_DOMAIN, LOG_TAG, __VA_ARGS__)

// å¤–éƒ¨ Sherpa å‡½æ•°å£°æ˜
extern napi_value InitSherpa(napi_env env, napi_callback_info info);
extern napi_value AcceptWaveform(napi_env env, napi_callback_info info);
extern napi_value ResetSherpa(napi_env env, napi_callback_info info);
extern napi_value GetRecognizedText(napi_env env, napi_callback_info info);
extern napi_value GetQueueSize(napi_env env, napi_callback_info info);

// ==========================================
// LLM å¼‚æ­¥åŒ–æ ¸å¿ƒå˜é‡
// ==========================================
static llama_model* g_model = nullptr;
static llama_context* g_ctx = nullptr;

// çº¿ç¨‹å®‰å…¨æ§åˆ¶
static std::mutex g_llm_mutex;
static std::string g_llm_input_prompt = "";   // å¾…å¤„ç†çš„é—®é¢˜
static std::string g_llm_output_buffer = "";  // å¾…å–èµ°çš„ç­”æ¡ˆ
static std::atomic<bool> g_llm_running = false;
static std::thread* g_llm_thread = nullptr;

// ğŸ”¥ LLM åå°å·¥ä½œçº¿ç¨‹ ğŸ”¥
void LlmBackgroundWorker() {
    LOGI("ğŸ§µ LLM åå°çº¿ç¨‹å·²å¯åŠ¨");
    while (g_llm_running) {
        std::string prompt;
        {
            std::lock_guard<std::mutex> lock(g_llm_mutex);
            if (!g_llm_input_prompt.empty()) {
                prompt = g_llm_input_prompt;
                g_llm_input_prompt = ""; // å–èµ°ä»»åŠ¡
            }
        }

        if (prompt.empty()) {
            usleep(20000); // æ²¡ä»»åŠ¡å°±ä¼‘æ¯ 20ms
            continue;
        }

        if (!g_model || !g_ctx) {
            LOGE("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æ¨ç†");
            continue;
        }

        // --- å¼€å§‹æ¨ç† (è€—æ—¶æ“ä½œ) ---
        LOGI("ğŸ¤– LLM å¼€å§‹æ€è€ƒ: %{public}s", prompt.c_str());
        
        // 1. Tokenize
        std::string full_prompt = "<|im_start|>user\n" + prompt + "<|im_end|>\n<|im_start|>assistant\n";
        const llama_vocab* vocab = llama_model_get_vocab(g_model);
        std::vector<llama_token> tokens(full_prompt.length() + 100);
        int n_tokens = llama_tokenize(vocab, full_prompt.c_str(), full_prompt.length(), tokens.data(), tokens.size(), true, true);
        if (n_tokens < 0) {
            n_tokens = -n_tokens;
            tokens.resize(n_tokens);
            n_tokens = llama_tokenize(vocab, full_prompt.c_str(), full_prompt.length(), tokens.data(), tokens.size(), true, true);
        }
        tokens.resize(n_tokens);

        // 2. Initial Decode
        llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());
        llama_decode(g_ctx, batch);

        // 3. Generation Loop
        for (int i = 0; i < 512; i++) { // æœ€å¤šç”Ÿæˆ 512 token
            auto * logits = llama_get_logits_ith(g_ctx, batch.n_tokens - 1);
            int n_vocab = llama_vocab_n_tokens(vocab);
            
            llama_token next_token = 0;
            float max_p = -1e9;
            for (int j = 0; j < n_vocab; j++) {
                if (logits[j] > max_p) {
                    max_p = logits[j];
                    next_token = j;
                }
            }

            // é‡åˆ°ç»“æŸç¬¦åœæ­¢
            if (llama_vocab_is_eog(vocab, next_token)) break;

            // è½¬ä¸ºå­—ç¬¦ä¸²
            char buf[256];
            int n = llama_token_to_piece(vocab, next_token, buf, sizeof(buf), 0, true);
            if (n < 0) {
                 n = -n;
                 llama_token_to_piece(vocab, next_token, buf, n, 0, true);
            }
            buf[n] = '\0';

            // ğŸ”¥ å°†ç”Ÿæˆçš„å­—æ”¾å…¥ç¼“å†²åŒºï¼Œä¾› JS æ‹¿å– ğŸ”¥
            {
                std::lock_guard<std::mutex> lock(g_llm_mutex);
                g_llm_output_buffer += std::string(buf);
            }

            // å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£
            batch = llama_batch_get_one(&next_token, 1);
            if (llama_decode(g_ctx, batch) != 0) break;
        }
        
        LOGI("âœ… LLM å›å¤å®Œæˆ");
    }
}

// 1. åŠ è½½ LLM (åŒæ—¶å¯åŠ¨åå°çº¿ç¨‹)
static napi_value NativeLoad(napi_env env, napi_callback_info info) {
    size_t argc = 1;
    napi_value args[1];
    napi_get_cb_info(env, info, &argc, args, nullptr, nullptr);
    char pathBuf[512];
    size_t strSize;
    napi_get_value_string_utf8(env, args[0], pathBuf, 512, &strSize);

    if (g_ctx) { llama_free(g_ctx); g_ctx = nullptr; }
    if (g_model) { llama_free_model(g_model); g_model = nullptr; }

    llama_backend_init();
    llama_model_params model_params = llama_model_default_params();
    model_params.use_mmap = false; 

    g_model = llama_model_load_from_file(pathBuf, model_params);
    bool success = (g_model != nullptr);
    
    if (success) {
        llama_context_params ctx_params = llama_context_default_params();
        ctx_params.n_ctx = 2048;
        ctx_params.n_threads = 4;
        ctx_params.n_threads_batch = 4;
        ctx_params.n_batch = 128; 
        g_ctx = llama_new_context_with_model(g_model, ctx_params);
        
        // ğŸ”¥ å¯åŠ¨åå°çº¿ç¨‹ ğŸ”¥
        if (!g_llm_running) {
            g_llm_running = true;
            g_llm_thread = new std::thread(LlmBackgroundWorker);
            g_llm_thread->detach();
        }
    }

    napi_value result;
    napi_get_boolean(env, success, &result);
    return result;
}

// 2. å‘é€é—®é¢˜ (éé˜»å¡ï¼Œç«‹å³è¿”å›)
static napi_value NativeChat(napi_env env, napi_callback_info info) {
    size_t argc = 1; 
    napi_value args[1];
    napi_get_cb_info(env, info, &argc, args, nullptr, nullptr);

    char qBuf[1024];
    size_t strSize;
    napi_get_value_string_utf8(env, args[0], qBuf, 1024, &strSize);
    
    // åªè´Ÿè´£æŠŠé—®é¢˜æ”¾å…¥é˜Ÿåˆ—
    {
        std::lock_guard<std::mutex> lock(g_llm_mutex);
        g_llm_input_prompt = std::string(qBuf);
    }

    napi_value result;
    napi_create_string_utf8(env, "OK", NAPI_AUTO_LENGTH, &result);
    return result;
}

// 3. è·å–ç»“æœ (ä¾› JS è½®è¯¢)
static napi_value GetLlmResult(napi_env env, napi_callback_info info) {
    std::string res = "";
    {
        std::lock_guard<std::mutex> lock(g_llm_mutex);
        if (!g_llm_output_buffer.empty()) {
            res = g_llm_output_buffer;
            g_llm_output_buffer = ""; // å–èµ°åæ¸…ç©ºï¼Œå®ç°æµå¼
        }
    }
    napi_value output;
    napi_create_string_utf8(env, res.c_str(), NAPI_AUTO_LENGTH, &output);
    return output;
}

EXTERN_C_START
static napi_value Init(napi_env env, napi_value exports) {
    napi_property_descriptor desc[] = {
        // LLM
        {"nativeLoad", nullptr, NativeLoad, nullptr, nullptr, nullptr, napi_default, nullptr},
        {"nativeChat", nullptr, NativeChat, nullptr, nullptr, nullptr, napi_default, nullptr},
        {"getLlmResult", nullptr, GetLlmResult, nullptr, nullptr, nullptr, napi_default, nullptr}, // æ–°å¢æ¥å£
        
        // Sherpa
        {"initSherpa", nullptr, InitSherpa, nullptr, nullptr, nullptr, napi_default, nullptr},
        {"acceptWaveform", nullptr, AcceptWaveform, nullptr, nullptr, nullptr, napi_default, nullptr},
        {"resetSherpa", nullptr, ResetSherpa, nullptr, nullptr, nullptr, napi_default, nullptr},
        {"getRecognizedText", nullptr, GetRecognizedText, nullptr, nullptr, nullptr, napi_default, nullptr},
        {"getQueueSize", nullptr, GetQueueSize, nullptr, nullptr, nullptr, napi_default, nullptr}
    };
    napi_define_properties(env, exports, sizeof(desc) / sizeof(desc[0]), desc);
    return exports;
}
EXTERN_C_END

static napi_module demoModule = {
    .nm_version = 1,
    .nm_flags = 0,
    .nm_filename = nullptr,
    .nm_register_func = Init,
    .nm_modname = "mnnllm",
    .nm_priv = ((void*)0),
    .reserved = { 0 },
};

extern "C" __attribute__((constructor)) void RegisterEntryModule(void) {
    napi_module_register(&demoModule);
}